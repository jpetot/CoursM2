install.packages("emmeans")
require(emmeans)
anova(modele.1way)
pairwise.t.test(fich_eau$intensite.gustative.globale,fich_eau$produit,p.adjust="none")
pairwise.t.test(fich_eau$intensite.gustative.globale,fich_eau$produit,p.adjust="bonferroni")
modele.1way <- lm(intensite.gustative.globale ~ produit,data=fich_eau)
anova(modele.1way)
tuk <- glht(modele.1way,linfct=mcp(produit="Tukey"))
summary(tuk)
tuk.cld <- cld(tuk)
plot(tuk.cld)
mod.IGG <- lm(intensite.gustative.globale  produit,Eau)
mod.IGG <- lm(intensite.gustative.globale ~ produit, fich_eau)
emmeans(mod.IGG, pairwise  produit)
emmeans(mod.IGG, pairwise ~ produit)
summary(fich_eau)
fich_eau$juge = as.factor(fich_eau$juge)
# q4
mod.2eff<- lm(intensite.gustative.globale ~ produit + juge, fich_eau)
anova(mod.2eff)
require(car)
# q5
options(contrasts = c("contr.sum", "contr.sum"))
Anova(res.aov,type="III")
res.aov = anova(mod.2eff)
Anova(res.aov,type="III")
install.packages("car")
install.packages("car")
require(car)
# q5
options(contrasts = c("contr.sum", "contr.sum"))
Anova(res.aov,type="III")
Anova(mod.2eff,type="III")
Anova(mod.2eff,type="I")
anova(mod.2eff)
# q4
mod.2eff<- lm(intensite.gustative.globale ~ produit * juge, fich_eau)
res.aov = anova(mod.2eff)
# q5
options(contrasts = c("contr.sum", "contr.sum"))
Anova(mod.2eff,type="III")
anova(mod.2eff)
res3 = Anova(mod.2eff,type="III")
res1 = anova(mod.2eff)
# q4
mod.2eff<- lm(intensite.gustative.globale ~ produit + juge, fich_eau)
res.aov = anova(mod.2eff)
# q5
options(contrasts = c("contr.sum", "contr.sum"))
res3 = Anova(mod.2eff,type="III")
res1 = anova(mod.2eff)
# Q6
res3$`Sum Sq`
res1$`Sum Sq`
# q4
mod.2eff<- lm(intensite.gustative.globale ~ produit + juge, fich_eau)
res3 = Anova(mod.2eff,type="III")
res1 = anova(mod.2eff)
# Q6
res3$`Sum Sq`
res1$`Sum Sq`
# Q6
res3$`Sum Sq`
res1$`Sum Sq`
res.aov
res3
# Q6
sum(res3$`Sum Sq`)
sum(res1$`Sum Sq`)
summary(mod.2eff)
install.packages("phia")
require(phia)
#q8
mod.int<- lm(intensite.gustative.globale ~ produit * juge, fich_eau)
require(phia)
#q8
mod.inter <- lm(intensite.gustative.globale ~ produit * juge, fich_eau)
interactionMeans(mod.inter) ## Toutes les moyennes d'interaction
#q8
mod.inter <- lm(intensite.gustative.globale ~ produit * juge, fich_eau)
interactionMeans(mod.inter) ## Toutes les moyennes d'interaction
interactionMeans(mod.inter,factors="produit")## Uniquement les moyennes marginales pour le facteur produit
testInteractions(mod.inter,fixed="juge",across="produit") ## On teste l'effet produit sachant le juge
testInteractions(mod.inter)
testInteractions(mod.inter,pairwise="juge",across="produit")
setwd("~/M2/Apprentissage_statistique")
setwd("~/M2/Apprentissage_statistique")
install.packages("glmnet") # Installation is only needed if the package is missing
install.packages("leaps")
require(leaps)             # For variable selection
require(glmnet)            # For penalized regression procedures
require(fields)            # For image.plot
install.packages("fields")
require(fields)            # For image.plot
require(pls)               # For cvsegments, plsr, ...
pig = read.table("scanner.txt")
dim(pig)       # Numbers of rows and columns in dta
str(pig)       # Overview of the data table
matplot(1:137,t(pig[,1:137]),type="l",lty=1,col="orange",
main="CT curves for muscle percentage",xlab="Anatomical location",
ylab="Muscle %")
corxy = cor(pig[,1:137],pig[,138])
plot(1:137,corxy,type="l",col="orange",
main="Correlation between CT curves for muscle percentage and LMP",
xlab="Anatomical location",
ylab="Correlation")
## Correlation matrix across explanatory variables
xcor = cor(pig[,-138])
### Image plot of the correlations across explanatory variables
image.plot(1:137,1:137,xcor[,137:1],xlab="Indices of explanatory variables",
ylab="Indices of explanatory variables",
main="Image plot of the correlations across explanatory variables",
yaxt="n",cex.lab=1.25,cex.axis=1.25,cex.main=1.25)
## Rank the models according to their RSS (using package leaps)
select = summary(regsubsets(LMP~.,data=pig,nvmax=115,method="forward"))
head(select$which)
beta = rep(0,137)       # Initialize an empty vector of regression coefficients
mod = glm(LMP~.,data=pig[,c(select$which[5,-1],TRUE),drop=FALSE])
beta[select$which[5,-1]] = coef(mod)[-1]
coef(mod)
beta
coef(mod)
plot(1:137,beta,bty="l",type="b",pch=16,lwd=2,xlab="Indices of explanatory variables",
ylab="Regression coefficients",main="Regression coefficients",cex.lab=1.25,
cex.axis=1.25,cex.main=1.25)
mtext("Best model with 5 variables",cex=1.25)
beta = rep(0,137)      # Initialize an empty vector of regression coefficients
mod = glm(LMP~.,data=pig[,c(select$which[100,-1],TRUE),drop=FALSE])
beta[select$which[100,-1]] = coef(mod)[-1]
plot(1:137,beta,bty="l",type="b",pch=16,lwd=2,xlab="Indices of explanatory variables",
ylab="Regression coefficients",main="Regression coefficients",cex.lab=1.25,
cex.axis=1.25,cex.main=1.25)
mtext("Best model with 100 variables",cex=1.25)
## Matrix of explanatory variables
x = as.matrix(pig[,-138])
## Vector of response values
y = pig[,138]
## Decreasing sequence of log-lambda values
loglambda = seq(10,-10,length=100)
loglambda
## Fit ridge regression for a sequence of lambda
mod = glmnet(x,y,alpha=0,lambda=exp(loglambda))
## Initialize an empty matrix of predicted values
cvpred = matrix(0,nrow=nrow(pig),ncol=100)
## Create 10 random segments of the dataset
segments = cvsegments(nrow(pig),10)
cvpred
## Create 10 random segments of the dataset
segments = cvsegments(nrow(pig),10)
segments
for (k in 1:10) {
mod = glmnet(x[-segments[[k]],],y[-segments[[k]]],alpha=0,lambda=exp(loglambda))
cvpred[segments[[k]],] = predict(mod,newx=x[segments[[k]],])
}
## Predicted versus observed LMP with large lambda
plot(pig$LMP,cvpred[,1],pch=16,bty="n",xlab="Observed LMP",
ylab="Predicted LMP",main="Fitted versus observed LMP",
cex.lab=1.25,cex.axis=1.25,cex.main=1.25,ylim=range(pig$LMP))
mtext(expression(Large~lambda),cex=1.25)
abline(0,1,lwd=2,col="darkgray")
## Predicted versus observed LMP with small lambda
plot(pig$LMP,cvpred[,100],pch=16,bty="n",xlab="Observed LMP",
ylab="Predicted LMP",main="Fitted versus fitted LMP",
cex.lab=1.25,cex.axis=1.25,cex.main=1.25,ylim=range(pig$LMP))
mtext(expression(lambda~close~to~zero),cex=1.25)
abline(0,1,lwd=2,col="darkgray")
## Searching for the best penalty parameter
cvmod = cv.glmnet(x,y,alpha=0,lambda=exp(loglambda))
## MSEP profile in ridge regression
plot(cvmod)
## Take the predictions using the optimal lambda value
pred = cvpred[,which.min(cvmod$cvm)]
## Fitted versus observed LMP
plot(pig$LMP,pred,pch=16,bty="n",xlab="Observed LMP",
ylab="Predicted LMP",main="Fitted versus fitted LMP",
cex.lab=1.25,cex.axis=1.25,cex.main=1.25,ylim=range(pig$LMP))
mtext(expression(Optimal~lambda),cex=1.25)
abline(0,1,lwd=2,col="darkgray")
### Initialize an empty matrix of predicted values
cvpred = matrix(0,nrow=nrow(pig),ncol=100)
### Create 10 random segments of the dataset
segments = cvsegments(nrow(pig),10)
for (k in 1:10) {
trainx = x[-segments[[k]],]
trainy = y[-segments[[k]]]
testx = x[segments[[k]],]
cvmod = cv.glmnet(trainx,trainy,alpha=0,lambda=exp(loglambda))
mod = glmnet(trainx,trainy,alpha=0,lambda=exp(loglambda))
cvpred[segments[[k]],] = predict(mod,newx=testx)[,which.min(cvmod$cvm)]
print(paste("Segment ",k,sep=""))
}
MSEP = mean((pig$LMP-cvpred)^2)
## Searching for the best penalty parameter
cvmod = cv.glmnet(x,y,alpha=0,lambda=exp(loglambda))
## MSEP profile in ridge regression
plot(cvmod,ylim=c(1,5))
abline(h=MSEP,col="orange",lwd=2)
x = pig$X92
y = pig$LMP
n = nrow(pig) ; n
sxy = cov(x,y) ; sxy
s2x = var(x)
vlambda = seq(0,150,length=1000)
beta = (sxy-vlambda/(2*n))/s2x
beta[sxy<=vlambda/(2*n)] = 0
# Estimated regression coefficient along with lambda
plot(vlambda,beta,type="l",lwd=2,bty="n",xlab=expression(lambda),
ylab=expression(hat(beta)),main="Estimated regression coefficient using LASSO",
cex.lab=1.25,cex.axis=1.25,cex.main=1.25,col="darkgray")
### Matrix of explanatory variables
x = as.matrix(pig[,-138])
### Vector of response values
y = pig[,138]
### Decreasing sequence of log-lambda values
loglambda = seq(10,-10,length=100)
### Fit LASSO regression for a sequence of lambda
mod = glmnet(x,y,alpha=1,lambda=exp(loglambda))
### Initialize an empty matrix of predicted values
cvpred = matrix(0,nrow=nrow(pig),ncol=100)
### Create 10 random segments of the dataset
segments = cvsegments(nrow(pig),10)
for (k in 1:10) {
mod = glmnet(x[-segments[[k]],],y[-segments[[k]]],alpha=1,lambda=exp(loglambda))
cvpred[segments[[k]],] = predict(mod,newx=x[segments[[k]],])
}
cvmod = cv.glmnet(x,y,alpha=1,lambda=exp(loglambda))
### MSEP profile in ridge regression
plot(cvmod)
### Take the predictions using the optimal lambda value
pred = cvpred[,which.min(cvmod$cvm)]
### Fitted versus observed LMP
plot(pig$LMP,pred,pch=16,bty="n",xlab="Observed LMP",
ylab="Predicted LMP",main="Fitted versus fitted LMP",
cex.lab=1.25,cex.axis=1.25,cex.main=1.25,ylim=range(pig$LMP))
mtext(expression(Optimal~lambda),cex=1.25)
abline(0,1,lwd=2,col="darkgray")
### Sparsity of the estimated regression model
mod = glmnet(x,y,alpha=1,lambda=exp(loglambda))
plot(1:137,mod$beta[,which.min(cvmod$cvm)],pch=16,bty="n",xlab="Indices of explanatory variables",
ylab=expression(hat(beta)),main="Estimated regression coefficients using LASSO",
cex.lab=1.25,cex.axis=1.25,cex.main=1.25)
mtext(expression(Optimal~lambda),cex=1.25)
#### Initialize an empty matrix of predicted values
cvpred = matrix(0,nrow=nrow(pig),ncol=100)
### Matrix of explanatory variables
x = as.matrix(pig[,-138])
### Vector of response values
y = pig[,138]
### Decreasing sequence of log-lambda values
loglambda = seq(10,-10,length=100)
### Fit LASSO regression for a sequence of lambda
mod = glmnet(x,y,alpha=1,lambda=exp(loglambda))
### Initialize an empty matrix of predicted values
cvpred = matrix(0,nrow=nrow(pig),ncol=100)
### Create 10 random segments of the dataset
segments = cvsegments(nrow(pig),10)
for (k in 1:10) {
mod = glmnet(x[-segments[[k]],],y[-segments[[k]]],alpha=1,lambda=exp(loglambda))
cvpred[segments[[k]],] = predict(mod,newx=x[segments[[k]],])
}
cvmod = cv.glmnet(x,y,alpha=1,lambda=exp(loglambda))
### MSEP profile in ridge regression
plot(cvmod)
### Take the predictions using the optimal lambda value
pred = cvpred[,which.min(cvmod$cvm)]
### Fitted versus observed LMP
plot(pig$LMP,pred,pch=16,bty="n",xlab="Observed LMP",
ylab="Predicted LMP",main="Fitted versus fitted LMP",
cex.lab=1.25,cex.axis=1.25,cex.main=1.25,ylim=range(pig$LMP))
mtext(expression(Optimal~lambda),cex=1.25)
abline(0,1,lwd=2,col="darkgray")
### Sparsity of the estimated regression model
mod = glmnet(x,y,alpha=1,lambda=exp(loglambda))
plot(1:137,mod$beta[,which.min(cvmod$cvm)],pch=16,bty="n",xlab="Indices of explanatory variables",
ylab=expression(hat(beta)),main="Estimated regression coefficients using LASSO",
cex.lab=1.25,cex.axis=1.25,cex.main=1.25)
mtext(expression(Optimal~lambda),cex=1.25)
#### Initialize an empty matrix of predicted values
cvpred = matrix(0,nrow=nrow(pig),ncol=100)
#### Create 10 random segments of the dataset
segments = cvsegments(nrow(pig),10)
for (k in 1:10) {
trainx = x[-segments[[k]],]
trainy = y[-segments[[k]]]
testx = x[segments[[k]],]
cvmod = cv.glmnet(trainx,trainy,alpha=1,lambda=exp(loglambda))
mod = glmnet(trainx,trainy,alpha=1,lambda=exp(loglambda))
cvpred[segments[[k]],] = predict(mod,newx=testx)[,which.min(cvmod$cvm)]
print(paste("Segment ",k,sep=""))
}
MSEP = mean((pig$LMP-cvpred)^2)
## Searching for the best penalty parameter
cvmod = cv.glmnet(x,y,alpha=1,lambda=exp(loglambda))
## MSEP profile in lasso regression
plot(cvmod,ylim=c(1,5))
abline(h=MSEP,col="orange",lwd=2)
coffee_nirs = read.table("./Data/coffee_nirs.txt")
coffee_nirs$Localisation = factor(coffee_nirs$Localisation)
dim(coffee_nirs)
coffee_nirs = read.table("coffee_nirs.txt")
coffee_nirs = read.table("https://tice.agrocampus-ouest.fr/pluginfile.php/95372/mod_folder/intro/coffee_nirs.txt")
coffee_nirs = read.table("coffee_nirs.txt")
coffee_nirs$Localisation = factor(coffee_nirs$Localisation)
dim(coffee_nirs)
str(coffee_nirs[,1:15])
x = coffee_nirs[,-(1:6)]    # NIRS data
snv_x = t(scale(t(x)))      # SNV-transformed NIRS
wn = seq(402,2500,2)           # Wavenumbers
matplot(wn,t(snv_x),type="l",lwd=2,col="orange",lty=1,
bty="l",xlab=expression(Wave~numbers~(nm^-1)),
ylab="SNV-transformed NIRS",main="NIRS data of coffee samples",
cex.main=1.25,cex.lab=1.25,cex.axis=1.25)
y = coffee_nirs$Localisation
loglambda = seq(-10,-1,length=100)
coffee_nirs.cvlasso = cv.glmnet(snv_x,y,family="multinomial",type.measure="deviance",
lambda=exp(loglambda))
plot(coffee_nirs.cvlasso)
coffee_nirs.lasso = glmnet(snv_x,y,family="multinomial",lambda=exp(loglambda))
proba = predict(coffee_nirs.lasso,newx=snv_x,
type="response")[,,which.min(coffee_nirs.cvlasso$cvm)]
head(proba)   # For each coffee, estimated class probabilities
predictions = predict(coffee_nirs.lasso,newx=snv_x,
type="class")[,which.min(coffee_nirs.cvlasso$cvm)]
head(predictions)   # For each coffee, predicted class using Bayes rule
confusion = table(coffee_nirs$Localisation,predictions)
acc = mean(coffee_nirs$Localisation==predictions)
acc
dta = data.frame(snv_x,"Localisation"=y)
dim(dta)
segs = fold(dta,k=10,cat_col="Localisation")$".folds"
require(pls)               # For cvsegments, plsr, ...
require(fields)            # For image.plot
require(leaps)             # For variable selection
require(glmnet)            # For penalized regression procedures
segs = fold(dta,k=10,cat_col="Localisation")$".folds"
cvpredictions = rep("0",nrow=nrow(dta))
for (k in 1:10) {
train = dta[segs!=k,]
test = dta[segs==k,]
dta.cvlasso = cv.glmnet(as.matrix(train[,-1051]),train[,1051],family="multinomial",
type.measure="deviance",lambda=exp(loglambda))
dta.lasso = glmnet(as.matrix(train[,-1051]),train[,1051],family="multinomial",
lambda=exp(loglambda))
cvpredictions[segs==k] = predict(dta.lasso,newx=as.matrix(test[,-1051]),
type="class")[,which.min(dta.cvlasso$cvm)]
print(paste("Segment ",k," over 10",sep=""))
}
proba = predict(coffee_nirs.lasso,newx=snv_x,
type="response")[,,which.min(coffee_nirs.cvlasso$cvm)]
head(proba)   # For each coffee, estimated class probabilities
predictions = predict(coffee_nirs.lasso,newx=snv_x,
type="class")[,which.min(coffee_nirs.cvlasso$cvm)]
head(predictions)   # For each coffee, predicted class using Bayes rule
confusion = table(coffee_nirs$Localisation,predictions)
acc = mean(coffee_nirs$Localisation==predictions)
acc
confusion
dta = data.frame(snv_x,"Localisation"=y)
dim(dta)
segs = fold(dta,k=10,cat_col="Localisation")$".folds"
?fold
??fold
bactrain = read.table("https://tice.agrocampus-ouest.fr/pluginfile.php/95372/mod_folder/intro/bacttrain.txt", header = TRUE)
bactrain = read.table("https://tice.agrocampus-ouest.fr/pluginfile.php/95372/mod_folder/intro/bacttrain.txt", header = TRUE, sep = "\t")
summary(bactrain)
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
```{r cars}
summary(cars)
bactrain = read.table("bacttrain.txt")
summary(bactrain)
bacttest = read.table("bacttest.txt")
summary(bacttest)
levels(bactrain$Type)
bactrain
levels(bacttrain$Type)
bacttrain = read.table("bacttrain.txt")
bacttest = read.table("bacttest.txt")
summary(bactrain)
summary(bacttrain)
summary(bacttest)
levels(bacttrain$Type)
bacttrain$Type = as.factor(bacttrain$Type)
levels(bacttrain$Type)
bacttrain = read.table("bacttrain.txt", encoding = "latin1")
bacttest = read.table("bacttest.txt", encoding = "latin1")
summary(bacttrain)
summary(bacttest)
bacttrain$Type = as.factor(bacttrain$Type)
levels(bacttrain$Type)
bacttrain = read.table("bacttrain.txt")
bacttest = read.table("bacttest.txt"")
summary(bacttrain)
bacttrain = read.table("bacttrain.txt")
bacttrain = read.table("bacttrain.txt")
bacttest = read.table("bacttest.txt")
summary(bacttrain)
summary(bacttest)
bacttrain$Type = as.factor(bacttrain$Type)
levels(bacttrain$Type)
bacttest$Type = as.factor(bacttest$Type)
levels(bacttrain$Type)
levels(bacttest$Type)
mod_bi = glm(Type ~. , data = bacttrain, family = "Binomial" )
?glm
mod_bi = glm(Type ~. , data = bacttrain, family = binomial )
select = stepwise(mod_bi,direction="forward/backward",criterion="AIC")
require(RcmdrMisc)       # For Stepwise
mod_bi = glm(Type ~. , data = bacttrain, family = binomial)
select = stepwise(mod_bi,direction="forward/backward",criterion="AIC")
select = stepwise(mod_bi,direction="forward/backward",criterion="AIC", trace = FALSE)
select
proba = predict(mod_bi, newdata = bacttest, type = "response")
proba
proba2 = predict(select, newdata = bacttest, type = "response")
proba2
plot(proba2)
plot(proba)
levels(bacttest$Type)
prediction = ifelse(proba>= 0.5, "Positif", "Négatif")
prediction = ifelse(proba>= 0.5, "Positif", "Négatif")
table(bacttest$Type, prediction)
mean(bacttest$Type == prediction)
dim(bacttrain)
x = as.matrix(bacttrain[,-31])
y = bacttrain[,31]
loglambda = seq(10,-10,length=100)
### Fit LASSO regression for a sequence of lambda
mod = glmnet(x,y,alpha=1,lambda=exp(loglambda))
require(glmnet)            # For penalized regression procedures
x = as.matrix(bacttrain[,-31])
y = bacttrain[,31]
loglambda = seq(10,-10,length=100)
### Fit LASSO regression for a sequence of lambda
mod = glmnet(x,y,alpha=1,lambda=exp(loglambda))
mean(bacttest$Type == prediction) # accuracy
prediction = ifelse(proba2>= 0.5, "Positif", "Négatif") #debat sur le choix du seuil, on peut le faire varier et trouver un  compromis
table(bacttest$Type, prediction)
mean(bacttest$Type == prediction) # accuracy
### Fit LASSO regression for a sequence of lambda
mod = glmnet(x,y,alpha=1,lambda=exp(loglambda))
### Fit LASSO regression for a sequence of lambda
mod = glmnet(x,y,alpha=1,lambda=exp(loglambda), family = binomial)
prediction_lasso =  predict(mod, newdata = bacttest, type = "response")
prediction_lasso = ifelse(prob_lasso>= 0.5, "Positif", "Négatif")
x = as.matrix(bacttrain[,-31])
y = bacttrain[,31]
loglambda = seq(10,-10,length=100)
### Fit LASSO regression for a sequence of lambda
mod = glmnet(x,y,alpha=1,lambda=exp(loglambda), family = binomial)
prob_lasso =  predict(mod, newdata = bacttest, type = "response")
prediction_lasso = ifelse(prob_lasso>= 0.5, "Positif", "Négatif")
prob_lasso =  predict(mod, newx = bacttest, type = "response")
prob_lasso =  predict(mod[,-31], newx = bacttest, type = "response")
dim(bacttest)
prob_lasso =  predict(mod, newx = bacttest[,-31], type = "response")
prob_lasso =  predict(mod, newx = bacttest[,-31])
prob_lasso =  predict(mod, newx = as.matrix(bacttest[,-31]))
prediction_lasso = ifelse(prob_lasso>= 0.5, "Positif", "Négatif")
prediction_lasso
prediction
table(bacttest$Type, prediction_lasso)
bacttest$Type
prediction_lasso
vecs = seq(0.01,0.99,0.01)
acc = rep(0, length(vecs))
tfp = rep(0, length(vecs))
tfn = rep(0, length(vecs))
for (k in 1:length(vecs)){
prediction = ifelse(proba2>= vecs[k], "Positif", "Négatif")
acc[k] = mean(bacttest$Type == prediction)
t = table(bacttest$Type, prediction)
tfp[k] = t[1,2]/(t[1,2] + t[1,1])
tfn[k] = t[2,1]/(t[2,2] + t[2,1])
}
plot(tfp, vecs)
p1 = plot( vecs, tfp)
p2 = plot( vecs, tfn)
coplot(p1,p2)
require(coplot)
install.packages("coplot")
require(coplot)
require(ggplot2)
install.packages("ggplot2")
install.packages("ggplot2")
require(ggplot2)
lines(vecs,tfp,col="green")
plot(tfn,vecs,type="l",col="red")
plot(vecs,tfn,type="l",col="red")
lines(vecs,tfp,col="green")
prob_lasso =  predict(mod, newx = as.matrix(bacttest[,-31]))
prob_lasso =  predict(mod, newx = as.matrix(bacttest[,-31]), type = "response")
type(bacttest$Type)
str(bacttest)
### Fit LASSO regression for a sequence of lambda
mod = glmnet(x,y,alpha=1,lambda=exp(loglambda), family = binomial)
require(glmnet)            # For penalized regression procedures
require(ggplot2)
require(RcmdrMisc)       # For Stepwise
### Fit LASSO regression for a sequence of lambda
mod = glmnet(x,y,alpha=1,lambda=exp(loglambda), family = binomial)
prob_lasso =  predict(mod, newx = as.matrix(bacttest[,-31]), type = "response")
prediction_lasso = ifelse(prob_lasso>= 0.5, "Positif", "Négatif")
table(bacttest$Type, prediction_lasso)
prediction_lasso
class(prediction_lasso)
class(bacctest$Type)
class(bacttest$Type)
dim(prediction_lasso)
dim(bacttest$Type)
dim(bacttest)
plot(prediction_lasso)
